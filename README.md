# Policyscope: офлайн‑оценка рекомендательных систем

**Policyscope** — это набор инструментов для сравнения рекомендательных моделей в офлайн‑режиме.
Он позволяет оценивать, насколько новая политика (модель **B**) могла бы улучшить целевую метрику по сравнению с текущей политикой **A**, используя лишь исторические логи **A**.
Вместо рискованного A/B‑теста в проде вы получаете приближенную оценку среднего эффекта и доверительные интервалы.

## Почему офлайн‑оценка важна?

Когда мы заменяем одну рекомендательную модель на другую, нас интересует средний прирост метрики (например, вероятность принятия предложения или долгосрочная выручка CLTV).
Однако запуск онлайн‑эксперимента может быть дорогим или рискованным. Поэтому исследователи используют методы **counterfactual evaluation**: мы пере‑взвешиваем наблюдённые отклики, чтобы ответить на вопрос: «что бы произошло, если бы пользователь увидел рекомендации другой модели?». 

В основе лежат следующие идеи:

- **Inverse Propensity Scoring (IPS)**: взвешивает вклад каждого лога по отношению вероятностей выбора в новой и старой политике. Если логирующая политика показала действие с вероятностью \(\pi_A(a\mid x)\), а целевая политика выбирает его с вероятностью \(\pi_B(a\mid x)\), то вес \(w_i\) для этого лога равен \(\pi_B(a\mid x)/\pi_A(a\mid x)\). Такое пере‑взвешивание даёт несмещённую оценку среднего исхода, если у логирующей политики были ненулевые вероятности для всех действий【600380035352962†L65-L104】.
- **Self‑Normalized IPS (SNIPS)**: нормализует суммы весов, чтобы уменьшить разброс и избежать завышенных оценок. SNIPS показал лучшую точность на синтетических данных по сравнению с обрезкой весов【600380035352962†L153-L178】.
- **Doubly Robust (DR)**: сочетает модель исхода (Direct Method) и IPS‑поправку. DR сохраняет несмещённость, если корректно оценены либо пропенсити, либо модель отклика, и обычно имеет меньшую дисперсию【754398002539179†L48-L63】.

Однако все методы требуют, чтобы логирующая политика давала ненулевой шанс каждому действию. В противном случае некоторые действия новой политики останутся «невидимыми» в логах, и оценка будет невозможна. Эта проблема известна как **insufficient support**; один из способов решения — добавить небольшую случайность в выборку (ε‑greedy) или явно логировать вероятность каждого действия【600380035352962†L125-L133】.

## Возможности Policyscope

* **Синтетический генератор**: симуляция пользователей, банковских предложений и исходов (принятие/CLTV). Позволяет проверять состоятельность методов на «оракуле».
* **Политики A и B**: детерминированная greedy‑политика и стохастические ε‑greedy/softmax с корректными пропенсити.
* **Оценщики**: Replay, IPS, SNIPS, Direct Method (DM), Doubly Robust (DR). Для IPS/SNIPS/DR предусмотрена обрезка весов и расчёт эффективного размера выборки (ESS).
* **Кластерный бутстрэп**: оценки доверительных интервалов по пользователям/сессиям для значения политики и ATE.
* **Формирование отчёта**: текстовый вывод, какая модель лучше и с какой уверенностью; сохранение JSON с полной статистикой и CSV с логами.

## Установка

```bash
python -m venv .venv && source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

Либо установите как пакет:

```bash
pip install -e .
```

## Быстрый пример: синтетический эксперимент

В каталоге `examples` есть скрипт `run_synthetic_experiment.py`, который генерирует синтетические данные и сравнивает две политики.  Политика **A** генерирует логи с корректными пропенсити (ε‑greedy или softmax).  Политика **B** выдаёт рекомендации для тех же пользователей. Сравниваются отклик и CLTV‑H.

Пример запуска:

```bash
python examples/run_synthetic_experiment.py \
  --n_users 50000 \
  --seed 42 \
  --policyA epsilon_greedy --epsilon 0.15 \
  --policyB softmax --tau 0.7 \
  --horizon 90 \
  --weight_clip 20
```

После выполнения скрипт сохранит:

- `artifacts/logs_A.csv` — сгенерированные логи политики A;
- `artifacts/summary.json` — все оценки (Replay/IPS/SNIPS/DM/DR) для отклика и CLTV;
- `artifacts/diagnostics.json` — метаинформация (ESS, доля клиппинга, параметры политики);
- `artifacts/report.txt` — человекочитаемый отчёт: оценки V(A) и V(B), разность, доверительные интервалы и рекомендация, какая политика лучше.

Отдельно рассчитывается «оракул» — истинные значения метрики под обеими политиками, что позволяет увидеть, насколько хорошо off‑policy оценка приближает результат онлайн‑эксперимента.

## Структура репозитория

```
OffPolicyLab/
├── LICENSE               # MIT
├── README.md            # данная инструкция и теория
├── requirements.txt
├── pyproject.toml
├── src/policyscope/     # исходный код пакета
│   ├── __init__.py
│   ├── synthetic.py      # генерация среды, пользователей и исходов
│   ├── policies.py       # определение политик (greedy, ε-greedy, softmax)
│   ├── estimators.py     # оценщики Replay/IPS/SNIPS/DM/DR и функции ESS
│   ├── bootstrap.py      # бутстрэп для доверительных интервалов
│   └── report.py         # формирование текстового отчёта и сохранение JSON
├── examples/
│   └── run_synthetic_experiment.py  # пример использования
├── tests/
│   └── test_placeholder.py         # базовый тест
└── .github/workflows/ci.yml        # CI
```

## Как использовать свои логи

Если у вас есть реальные логи политики A с пропенсити (вероятностями показа), вы можете загрузить их как DataFrame и воспользоваться `estimators.py` напрямую:

```python
import pandas as pd
from policyscope.estimators import train_mu_hat, prepare_piB_taken, ips_value, snips_value, dr_value
from policyscope.policies import make_policy

# df должно содержать: user_id, a_A (фактически показанный оффер), propensity_A, accept, cltv,
# а также признаки (loyal, age_z, risk_z, income_z). Для реальных данных вы можете создавать признаки сами.
df = pd.read_csv("your_logs_with_propensity.csv")
policyB = make_policy("softmax", tau=0.7)

piB_taken = prepare_piB_taken(df, policyB)  # вероятности, с которыми B показала бы логированные действия

# обучаем модель исхода для DR/DM
mu_accept = train_mu_hat(df, target="accept")

# оцениваем A и B
V_A = df["accept"].mean()
V_B_ips, ess_ips, clip_ips = ips_value(df, piB_taken, target="accept", weight_clip=20)
V_B_snips, ess_snips, clip_snips = snips_value(df, piB_taken, target="accept", weight_clip=20)
V_B_dr, ess_dr, clip_dr = dr_value(df, policyB, mu_accept, target="accept", weight_clip=20)

print("On‑policy V(A) =", V_A)
print("IPS V(B) =", V_B_ips)
print("SNIPS V(B) =", V_B_snips)
print("DR V(B) =", V_B_dr)
```

## Ссылки и материалы

* Евгений Ян. [Counterfactual Evaluation for Recommendation Systems](https://eugeneyan.com/writing/offline-recsys/) — обзор IPS, SNIPS и проблем insufficient support【600380035352962†L65-L104】【600380035352962†L125-L133】.
* Farajtabar et al. *More Robust Doubly Robust Off‑policy Evaluation* (arXiv:2205.13421) — обоснование doubly robust оценок и их свойств【754398002539179†L48-L63】.
* Наш генератор основан на аналогичных идеях synthetic‑логов из литературы по off‑policy evaluation.

Policyscope предоставляется по лицензии MIT. Если вы используете его в исследованиях или проде, будем рады ссылкам и контрибьюциям!